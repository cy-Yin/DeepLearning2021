吴恩达老师 2021年 深度学习 课程

deeplearning.ai 提供

[coursera](https://www.coursera.org/specializations/deep-learning?)

## **PART 01**: [Neural Networks and Deep Learning](https://www.coursera.org/learn/neural-networks-deep-learning?specialization=deep-learning)

### Week 01 Introduction to Deep Learning

- Welcome
- What is a Neural Network?
- Supervised Learning with Neural Networks
- Why is Deep Learning taking off?
- About this Course

*Practice quiz: Introduction to Deep Learning*

### Week 02 Neural Networks Basics

- Binary Classification
- Logistic Regression
- Logistic Regression Cost Function
- Gradient Descent
- Derivatives
- More Derivative Examples
- Computation Graph
- Derivatives with a Computation Graph
- Logistic Regression Gradient Descent
- Gradient Descent on m Examples
- Vectorization
- More Vectorization Examples
- Vectorizing Logistic Regression
- Vectorizing Logistic Regression's Gradient Output
- Broadcasting in Python
- A Note on Python/Numpy Vectors
- Quick tour of Jupyter/iPython Notebooks
- Explanation of Logistic Regression Cost Function (Optional)

*Practice quiz: Neural Network Basics*

### Week 03 Shallow Neural Networks

- Neural Networks Overview
- Neural Network Representation
- Computing a Neural Network's Output
- Vectorizing Across Multiple Examples
- Explanation for Vectorized Implementation
- Activation Functions
- Why do you need Non-Linear Activation Functions?
- Derivatives of Activation Functions
- Gradient Descent for Neural Networks
- Backpropagation Intuition (Optional)
- Random Initialization

*Practice quiz: Shallow Neural Networks*

### Week 04 Deep Neural Networks

- Deep L-layer Neural Network
- Forward Propagation in a Deep Network
- Getting your Matrix Dimensions Right
- Why Deep Representations?
- Building Blocks of Deep Neural Networks
- Forward and Backward Propagation
- Parameters vs Hyperparameters
- What does this have to do with the brain?

*Practice quiz: Key Concepts on Deep Neural Networks*

## **PART 02**: [Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization](https://www.coursera.org/learn/deep-neural-network?specialization=deep-learning)

### Week 05 Practical Aspects of Deep Learning

- Train / Dev / Test sets
- Bias / Variance
- Basic Recipe for Machine Learning
- Regularization
- Why Regularization Reduces Overfitting?
- Dropout Regularization
- Understanding Dropout
- Other Regularization Methods
- Normalizing Inputs
- Vanishing / Exploding Gradients
- Weight Initialization for Deep Networks
- Numerical Approximation of Gradients
- Gradient Checking
- Gradient Checking Implementation Notes

*Practice quiz: Practical aspects of Deep Learning*

### Week 06 Optimization Algorithms

- Mini-batch Gradient Descent
- Understanding Mini-batch Gradient Descent
- Exponentially Weighted Averages
- Understanding Exponentially Weighted Averages
- Bias Correction in Exponentially Weighted Averages
- Gradient Descent with Momentum
- RMSprop
- Adam Optimization Algorithm
- Learning Rate Decay
- The Problem of Local Optima

*Practice quiz: Optimization Algorithms*

### Week 07 Hyperparameter Tuning, Batch Normalization, Programming Frameworks

- Tuning Process
- Using an Appropriate Scale to pick Hyperparameters
- Hyperparameters Tuning in Practice: Pandas vs. Caviar
- Normalizing Activations in a Network
- Fitting Batch Norm into a Neural Network
- Why does Batch Norm work?
- Batch Norm at Test Time
- Softmax Regression
- Training a Softmax Classifier
- Deep Learning Frameworks
- TensorFlow

*Practice quiz: Hyperparameter tuning, Batch Normalization, Programming Frameworks*

## **PART 03**: [Structuring Machine Learning Projects](https://www.coursera.org/learn/machine-learning-projects?specialization=deep-learning)

### Week 08 ML Strategy

- Why ML Strategy
- Orthogonalization
- Single Number Evaluation Metric
- Satisficing and Optimizing Metric
- Train/Dev/Test Distributions
- Size of the Dev and Test Sets
- When to Change Dev/Test Sets and Metrics?
- Why Human-level Performance?
- Avoidable Bias
- Understanding Human-level Performance
- Surpassing Human-level Performance
- Improving your Model Performance

*Practice quiz: Bird Recognition in the City of Peacetopia (Case Study)*

### Week 09 ML Strategy II

- Carrying Out Error Analysis
- Cleaning Up Incorrectly Labeled Data
- Build your First System Quickly, then Iterate
- Training and Testing on Different Distributions
- Bias and Variance with Mismatched Data Distributions
- Addressing Data Mismatch
- Transfer Learning
- Multi-task Learning
- What is End-to-end Deep Learning?
- Whether to use End-to-end Deep Learning

*Practice quiz: Autonomous Driving (Case Study)*

## **PART 04**: [Convolutional Neural Networks](https://www.coursera.org/learn/convolutional-neural-networks?specialization=deep-learning)

### Week 10 Foundations of Convolutional Neural Networks

- Computer Vision
- Edge Detection Example
- More Edge Detection
- Padding
- Strided Convolutions
- Convolutions Over Volume
- One Layer of a Convolutional Network
- Simple Convolutional Network Example
- Pooling Layers
- CNN Example
- Why Convolutions?

*Practice quiz: The Basics of ConvNets*

### Week 11 Deep Convolutional Models: Case Studies

- Why look at case studies?
- Classic Networks
- ResNets
- Why ResNets Work?
- Networks in Networks and 1x1 Convolutions
- Inception Network Motivation
- Inception Network
- MobileNet
- MobileNet Architecture
- EfficientNet
- Using Open-Source Implementation
- Transfer Learning
- Data Augmentation
- State of Computer Vision

*Practice quiz: Deep Convolutional Models*

### Week 12 Special Applications: Face recognition & Neural Style Transfer

- What is Face Recognition?
- One Shot Learning
- Siamese Network
- Triplet Loss
- Face Verification and Binary Classification
- What is Neural Style Transfer?
- What are deep ConvNets learning?
- Cost Function
- Content Cost Function
- Style Cost Function
- 1D and 3D Generalizations

*Practice quiz: Special Applications: Face Recognition & Neural Style Transfer*

## **PART 05**: [Sequence Models](https://www.coursera.org/learn/nlp-sequence-models?specialization=deep-learning)

### Week 13 Recurrent Neural Networks

- Why Sequence Models?
- Notation
- Recurrent Neural Network Model
- Backpropagation Through Time
- Different Types of RNNs
- Language Model and Sequence Generation
- Sampling Novel Sequences
- Vanishing Gradients with RNNs
- Gated Recurrent Unit (GRU)
- Long Short Term Memory (LSTM)
- Bidirectional RNN
- Deep RNNs

*Practice quiz: Recurrent Neural Networks*

### Week 14 Natural Language Processing & Word Embeddings

- Word Representation
- Using Word Embeddings
- Properties of Word Embeddings
- Embedding Matrix
- Learning Word Embeddings
- Word2Vec
- Negative Sampling
- GloVe Word Vectors
- Sentiment Classification
- Debiasing Word Embeddings

*Practice quiz: Natural Language Processing & Word Embeddings*

### Week 15 Sequence Models & Attention Mechanism

- Basic Models
- Picking the Most Likely Sentence
- Beam Search
- Refinements to Beam Search
- Error Analysis in Beam Search
- Bleu Score (Optional)
- Attention Model Intuition
- Attention Model
- Speech Recognition
- Trigger Word Detection

*Practice quiz: Sequence Models & Attention Mechanism*

### Week 16 Transformer Network

- Transformer Network Intuition
- Self-Attention
- Multi-Head Attention
- Transformer Network
- Conclusion and Thank You!

*Practice quiz: Transformers*

## POSTSCRIPT

Some materials and notes can be found in [MachineLearning2022](https://github.com/cy-Yin/MachineLearning2022) repository in [GitHub](https://github.com/) since the knowledge has not changed a lot and the lectures in [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning?)are similar to those in the [Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction?).

I will point it out where someone can find the similar notes instead of repeating it again in this repository. 

Please pull this repository to your computer with another repository [MachineLearning2022](https://github.com/cy-Yin/MachineLearning2022) and open them through **Obsidian** to use the links. 

Make sure the two folders are in the same Obsidian Vault and the two folders are under the same path in your computer.

该仓库中笔记部分会与 [MachineLearning2022](https://github.com/cy-Yin/MachineLearning2022) 中的笔记重复，在此不再重写一遍，而是链接到 [MachineLearning2022](https://github.com/cy-Yin/MachineLearning2022) 的仓库的具体笔记部分。

使用Obsidian来显示创建的块链接。
- 拉取两个仓库（[MachineLearning2022](https://github.com/cy-Yin/MachineLearning2022)和[DeepLearning2021](https://github.com/cy-Yin/DeepLearning2021)）到本地。
- 创建一个Obsidian仓库，将两个文件夹放到这个仓库中的同一文件路径下（最好直接移动到创建的Obsidian仓库中，如创建一个名为AndrewNg的仓库，最终目录如下）
```
AndrewNg\
	.obsidian
	MachineLearning2022	
	DeepLearning2021
```
- 打开Obisidian应用即可使用我创建的块链接